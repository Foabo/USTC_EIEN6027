## 1. 概要

### 1.1 为什么要做数据挖掘？

我们生活在大量数据日积月累的年代。分析这些数据是一种重要需求。数据的爆炸式增长，广泛可用和巨大数量使得我们的时代成为真正的数据时代。急需功能强大和通用的工具，以便从这些海量数据中发现有价值的信息，把这些数据转换成有组织的知识。这种需求导致了数据挖掘的诞生。

### 1.2 数据挖掘需要解决什么问题

分类与回归、聚类、关联规则、时序模式、偏差检测

### 1.3 数据挖掘的主要步骤

1. 数据清理
2. 数据集成
3. 数据选择
4. 数据变换
5. 数据挖掘
6. 模式评估
7. 知识表示

## 2. 数据的概念：统计描述、可视化、距离度量

### 2.1 数据的基本概念

- 标称属性（Nominal）：是一些符号或事物的名称。每个值代表某种类别、编码或状态，因此标称属性又被看做是分类的。

- 二元属性（Binary）：是一种标称属性，只有两个类别或状态：0 和 1。

- 序数属性（Ordinal）：其可能的值之间具有有意义的序或秩评定，但是相继值之间的差是未知的。例如小、中、大。

- 数值属性（Numeric）：是定量的，是可度量的量。用整数或实数值表示。
  - 区间标度属性（interval-scaled）：用相等的单位尺度度量。区间的值有序，可以为正、0、或负；没有真正的零点；可以计算差值、均值、中位数和众数。
  - 比率标度属性（ratio-scaled）：具有固定零点的数值属性，也就是说一个数可以是另一个数的倍数；此外这些值是有序的，可以计算差值、均值、中位数和众数。

- 离散数学和连续属性：
  - 离散属性具有有限或无限可数个值，可以用或者不用整数表示。
  - 如果属性不是离散的，则它是连续的。

### 2.2 数据常见的统计特征有哪些 分别是怎么计算

基本统计描述有三类

- 中心趋势度量：度量数据分布的中部或中心位置。讨论均值、中位数、众数、中列数
- 数据的散布：数据分散程度的度量。**极差**、**四分位差**、四分位数极差、盒图、**方差**、**标准差**
- 基于图形的可视化审视数据：分位数图、分位数-分位数图、直方图、散点图

#### 中心趋势度量

- **均值**

  ![](https://tva1.sinaimg.cn/large/008eGmZEgy1gmgj6uqly0j30ze0j6wm4.jpg)

- **中位数**

  奇数是中间的数，偶数是中间两个数的和。但是当观测的数量很大，中位数的计算开销就很大，所以我们可以计算中位数的近似值，

  ![](https://tva1.sinaimg.cn/large/008eGmZEgy1gmgnerfi2ij30jc03mwew.jpg)

  $L_1$ 是中位数的下界，$N$是整个数据集中值的个数，$\sum(freq)_l$是低于中位数区间的所有区间的频率和，$frea_median$是中位数区间的频率，而$width$是中位数区间的宽度。
  
- **众数（mode）**

  集合中出现的最频繁的值。具有一个、两个、三个众数的数据集合分别称为单峰的（unimodal）、双峰的（bimodal）和三峰的（trimodal）。具有两个或多个众数的数据集是多峰的，极端情况下，每个数据只出现一次，则没有众数。

- 中列数

  数据集中最大和最小值的平均值。

#### 度量数据散布

- **极差**

  max-min

- **四分位数**

  2-分位数是将数据分为两半，小于二分位数的数据最多有1/2，对应于中位数。**四分位数**用3个数据点把数据划分成4个相等的部分。q-分位数将数据分成100个大小相等的连贯集。中位数，四分位数和百分位数是使用最广泛的分位数。

- 四分位数极差（IOR）

  $IQR = Q_3 - Q_1$
  
  给出被数据中间一半所覆盖的范围
  
- **五数概括**

  min,Q1,中位数(Q2),Q3,max

- 盒图

  盒图体现了五数的概念

  ![](https://tva1.sinaimg.cn/large/008eGmZEgy1gmh6ofoeh1j30n20miq4a.jpg)

  盒的端点一般在四分位数上，使得盒的长度是四分位数极差IQR；中位数用盒内线标记，盒外两条线（称作胡须）延伸到最小值和最大值。

- **方差和标准差**

  方差如下公式

  ![](https://tva1.sinaimg.cn/large/008eGmZEgy1gmh6rtm56pj310c03odgc.jpg)

  标准差是方差的平方根，是发散性的度量。$\delta$  度量关于均值的发散，仅当选择均值作为中心度量使用。$\delta$越小，整体数据越靠近均值。

  

  #### 数据的基本统计描述图形显示

  - 分位数图

    对于一个序列数据$X$,设每个观测值$x_i(i = 1，···，N)$是按递增排序的,$x_1$最小，$x_N$最大，每个观测值与一个百分数$f_i$配对，指出**大约** $f_i  $ x  100% 的数据小于$x_i$,这里说大约是因为可能没有一个精确地值$f_i$

    >百分比0.25对应于四分位数Q1，百分比0.50对应于中位数，百分比0.75对应于Q3
    >
    >

    ![](https://tva1.sinaimg.cn/large/008eGmZEgy1gmh789j66hj30me0b4abg.jpg)

  - 分位数-分位数图

  - 直方图

  - 散点图

  

### 2.3 怎么度量距离和相似性

*标称属性和二元属性的邻近性度量*

 *数值属性：**Lp**距离*

#### 标称属性的近邻性度量

标称属性可以取两个或多个状态，设一个标称属性的状态数目为$M$

**相异性**

两个对象i和j之间的相异性可以根据不匹配率计算：
$$
d(i,j) =  \frac{p - m} {p}
$$
其中 $m$是匹配的数目，即$i$和$j$取值相同的状态的属性数,而 $p$ 是刻画对象的属性总数，由$d(i,j)$可以构造相异性矩阵。

**相似性**
$$
sim(i,j) = 1 - d(i,j) = \frac{m}{p}
$$

#### 二元属性的近邻性度量

考察*对称*和*非对称*二元属性刻画的对象间的相异性和相似性度量。

二元属性只有 0 和 1 的取值。

**相异性**

如果所有的二元被看成具有相同的权重，则我们可以得到一个两行两列的列联表。

![](https://tva1.sinaimg.cn/large/008eGmZEgy1gmhbnm8yp1j30lg08k3zk.jpg)

> 其中 q 是对象 i 和 j 都取 1 的属性数；
>
> r 是在对象 i 中取 1 、在对象 j 中取 0 的属性数；
>
> s 是在对象 i 中取 0、 在对象 j 中取 1 的属性数；
>
> t 是对象 i 和 j 都取 0 的属性数；
>
> 属性总数为 p， p = q + r + s + t

 基于对称二元属性的相异性叫做**对称二元相异性**，如果i 和 j是对称的，即它们同样重要，则 i 和 j 的相异性为
$$
d(i,j) = \frac{r+s}{q+r+s+t}
$$
对于非对称的二元属性，两个状态不是同样重要，认为两个都取1的情况比两个都取0的情况更有意义，这种二元属性经常被认为是（“一元的”）其中负匹配数t被认为是不重要的
$$
d(i,j) = \frac{r+s}{q+r+s}
$$
类似的，我们可以基于相似性而不是相异性来度量两个二元属性的差别，**非对称的二元相似性**可以用如下公式
$$
sim(i,j) = \frac{q}{q+r+s} = 1 - d(i,j)
$$
这里的$sim(i,j)$被称作 **Jaccard系数**



#### 数值属性的相异性

欧几里得距离、曼哈顿距离、闵可夫斯基距离

**欧几里得距离**

描述对象 i 、 j 之间的欧几里得距离
$$
d(i,j) = \sqrt{(x_{i1}-x_{j1})^2+(x_{i2}-x_{j2})^2+...+(x_{ip}-x_{jp})^2}
$$


加权欧式距离
$$
d(i,j) = \sqrt{w_1(x_{i1}-x_{j1})^2+w_2(x_{i2}-x_{j2})^2+...+w_p(x_{ip}-x_{jp})^2}
$$


**曼哈顿距离**
$$
d(i,j) = |x_{i1}-x_{j1}| +|x_{i2}-x_{j2}| + ... + |x_{ip}-x_{jp}|
$$


> 欧式距离和曼哈顿距离满足如下数学性质
>
> **非负性：**$d(i,j)>= 0 $ :距离是个非负的数值
>
> **同一性：**$d(i,j) = 0$：对象到自身的距离是0
>
> **对称性：**$d(i,j) = d(j,i)$：距离是对称函数
>
> **三角不等式：** $d(i,j)<= d(i,k)+d(k,j)$：从 i 到 j  的距离不会大于途径任何其他对象 k 的距离
>
> 满足这些条件的测度叫做**度量（metric）**
>
> 

**闵可夫斯基距离（Minkowski distance）**

是欧氏距离和曼哈顿距离的推广
$$
d(i,j) = \sqrt[h]{|x_{i1}-x_{j1}|^h+|x_{i2}-x_{j2}|^h+...+|x_{ip}-x_{jp}|^h}
$$
其中$h$是实数，$h>=1$,也称作 $L_p$范数，其中的p就是这里的h，只不过这里保留了p作为属性数, p=1时候表示曼哈顿距离（$L_1$范数），p=2表示欧氏距离（$L_2$范数）

**上确界距离（$L_{max}$,$L_{\infty}$范数和切比舍夫距离）**

是$h->\infty$是闵可夫斯基距离的推广
$$
d(i,j) = \lim_{h\rightarrow\infty}{(\sum_{f=1}^{p}|x_{if}-x_{jf}|^h)^{\frac{1}{h}}} = \max_f^p{|x_{if}-x_{jf}|}
$$
$L_{\infty}$范数又称一致范数

> 例如，使用相同的数据对象 $x_1 = (1,2)$ 和$x_2 = (3,5)$，这两个对象的最大值差为 5 - 2 = 3.这是两个对象的上确界距离。

**余弦相似度**
$$
sim(x,y) = \frac{x · y}{||x||\times||y||}
$$
其中，$||x||$是向量x的欧几里得范数$$ \sqrt{x_1^2+x_2^2+...+x_p^2}$$,即向量的长度。该度量基于向量x和y之间夹角的余弦，等于0意味着两个向量呈$90^o$夹角（正交），没有匹配。



## 3. 数据预处理

### 3.1 为什么要进行数据预处理

不正确、不完整和不一致的数据是现实世界的大型数据库和数据仓库的共同特点。数据预处理正是需要解决这样的数据质量问题。

### 3.2 数据清洗主要解决什么问题

现实世界的数据一般是不完整的、有噪声的和不一致的。数据清洗试图填充缺失的值、光滑噪声并识别离群点、纠正数据中的不一致。

### 3.3 数据规约

数据规约（data reduction ）技术可以用来得到数据集的规约表示，它小得多，但仍接近于保持数据的完整性。

数据规约策略包括

- 维规约（dimensionality reduction）减少所考虑的随机变量的属性的个数。维规约方法包括**小波变换**和**主成分分析**，它们把元数据变换或投影到较小的空间。属性子集选择是一种维规约方法，其中不相关、弱相关或冗余的属性或维被检测和删除。
- 数量规约（numerosity reduction）用替代的较小的数据表示形式替换元数据，这些技术可以使参数的也可以是非参数的。对于*参数方法*而言，使用模型估计数据，使得一般只需要存放模型参数，而不是实际数据，离群点可能要存放，例如回归和对数-线性模型。*非参数方法*包括直方图、聚类、抽样和数据立方体聚集。
- 数据压缩（data compression）使用变换，以便得到元数据的规约或者压缩表示。如果元数据能够从压缩后的数据重构而不损失信息，则该数据规约成为无损的，否则是有损的。维规约和数量规约也可以视为某种形式的数据压缩。

#### 小波变换

离散小波变换（DWT）是一种线性信号处理技术，用于数据向量$X$时，将它变换成不同的数值小波系数向量$X^{'}$，两个向量具有相同的长度。用于数据规约时，每个元组看成是n维数据向量，即$X = (x_1,x_2,...,x_n)$ ，描述n个数据库属性在元组上的n个测量值。

如果小波变换后的数据与元数据的长度相等，这种技术如何能够实现数据压缩？关键在于小波变换后的数据可以截短，就能保留近似的压缩数据。例如保留大于设定的某个阈值的所有小波系数，其他系数置0。

DWT与离散傅里叶变换（DFT）有密切关系。一般来说DWT是一种更好的有损压缩，能提供元数据更准确的近似。

离散小波变换的一般过程使用一种层次金字塔算法（pyramid algorithm），它在每次迭代时将数据减半，导致计算速度很快。

1. 输入数据向量长度L必须是2的整数幂，必要时补0，满足L>=n
2. 每个变换涉及应用两个函数。第一个使用某种数据光滑，如求和或加权平均，第二个进行加权查分，提取数据细节特征。
3. 两个函数作用于$X$中的数据点对，即做用户所有的测量对（$x_{2i},x_{2i+1}$）。这导致两个长度为L/2的数据集，一般而言，它们粪便代表输入数据的光滑后的版本或低频版本和它的高频内容
4. 两个函数递归地作用于前面循环得到的数据集，直到得到的结果数据集的长度为2.
5. 由以上迭代得到的数据集中选择的值被指定为数据变换的小波系数。

#### 主成分分析（PCA）

首先介绍方差过滤。如果一个特征的方差很小，则意味着这个特征上很可能有大量取值都相同（比如90%都是1，只有10%是0，甚至100%是1），那这一个特征的取值对样本而言就没有区分度，这种特征就不带有有效信息。从方差的这种应用就可以推断出，如果一个特征的方差很大，则说明这个特征上带有大量的信息。因此，在降维中，**PCA使用的信息量衡量指标，就是样本方差，又称可解释性方差，方差越大，特征所带的信息量越多**。

PCA作为矩阵分解算法的核心算法，其实没有太多参数，但不幸的是每个参数的意义和运用都很难，因为几乎每个参数都涉及到高深的数学原理。

教材《数据挖掘概念与技术中》进行了简短的定义

>基本过程如下：
>
> （1）对输入数据规范化，使得每个属性都落入相同的区间。此步有助于确保具有较大定义域的属性不会支配具有较小定义域的属性。
>
> （2）PCA计算k个标准正交向量，作为规范化输入数据的基。这些是单位向量，每一个方向都垂直于另一个。这些向量称为主成分。输入数据是主成分的线性组合。
>
> （3）对主成分按“重要性”或强度降序排列。主成分基本上充当数据的新坐标轴，提供关于方差的重要信息。也就是说，对坐标轴进行排序，使得第一个坐标轴显示数据的最大方差，第二个显示次大方差，如此下去。例如，图2-17显示原来映射到轴X1和X2的给定数据集的前两个主成分Y1和Y2。这一信息帮助识别数据中的分组或模式。
>
> （4）既然主成分根据“重要性”降序排列，就可以通过去掉较弱的成分（即方差较小）来归约数据的规模。使用最强的主成分，应当能够重构原数据的很好的近似
>
>

**参考自吴恩达《机器学习》**

在**PCA**中，我们要做的是找到一个方向向量（**Vector direction**），当我们把所有的数据都投射到该向量上时，我们希望投射平均均方误差能尽可能地小。方向向量是一个经过原点的向量，而投射误差是从特征向量向该方向向量作垂线的长度。

![a93213474b35ce393320428996aeecd9](https://tva1.sinaimg.cn/large/008eGmZEgy1gmhfaciecrj308c06lmx4.jpg)



下面给出主成分分析问题的描述：

问题是要将$n$维数据降至$k$维，目标是找到向量$u^{(1)}$,$u^{(2)}$,...,$u^{(k)}$使得总的投射误差最小。主成分分析与线性回顾的比较：

主成分分析与线性回归是两种不同的算法。主成分分析最小化的是投射误差（**Projected Error**），而线性回归尝试的是最小化预测误差。线性回归的目的是预测结果，而主成分分析不作任何预测。

![7e1389918ab9358d1432d20ed20f8142](https://tva1.sinaimg.cn/large/008eGmZEgy1gmhfb7mh63j30dw05qq34.jpg)

上图中，左边的是线性回归的误差（垂直于横轴投影），右边则是主要成分分析的误差（垂直于红线投影）。

**PCA**将$n$个特征降维到$k$个，可以用来进行数据压缩，如果100维的向量最后可以用10维来表示，那么压缩率为90%。同样图像处理领域的**KL变换**使用**PCA**做图像压缩。但**PCA** 要保证降维后，还要保证数据的特性损失最小。

**PCA**技术的一大好处是对数据进行降维的处理。我们可以对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。

**PCA**技术的一个很大的优点是，它是完全无参数限制的。在**PCA**的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。

但是，这一点同时也可以看作是缺点。如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高。

以下复习过程如有困惑直接不看吧。

> ##### 主成分分析算法
>
> **PCA** 减少$n$维到$k$维：
>
> 第一步是均值归一化。我们需要计算出所有特征的均值，然后令 $x_j= x_j-μ_j$。如果特征是在不同的数量级上，我们还需要将其除以标准差 $σ^2$。
>
> 第二步是计算**协方差矩阵**（**covariance matrix**）$Σ$：
> $\sum=\dfrac {1}{m}\sum^{n}_{i=1}\left( x^{(i)}\right) \left( x^{(i)}\right) ^{T}$
>
> 第三步是计算协方差矩阵$Σ$的**特征向量**（**eigenvectors**）:
> $$
> X\rightarrow QΣQ^{-1}
> $$
> 将特征矩阵X分解为以下三个矩阵，其中$Q$ 和 $Q^{-1}$是辅助的矩阵，Σ是一个对角矩阵，其对角线上的元素就是方差。。降维完成之后，PCA找到的每个新特征向量就叫做“主成分”，而被丢弃的特征向量被认为信息量很少，这些信息很可能就是噪音。
>
> 也可以我们可以利用**奇异值分解**（**singular value decomposition**）来求解，`[U, S, V]= svd(sigma)`。
>
> ![](https://tva1.sinaimg.cn/large/008eGmZEgy1gmhfdsy0hyj30b403u0th.jpg)
>
> $$Sigma=\dfrac {1}{m}\sum^{n}_{i=1}\left( x^{(i)}\right) \left( x^{(i)}\right) ^{T}$$
>
> ![](https://tva1.sinaimg.cn/large/008eGmZEgy1gmhfefwm50j30go05odia.jpg)
>
> 对于一个 $n×n$维度的矩阵，上式中的$U$是一个具有与数据之间最小投射误差的方向向量构成的矩阵。如果我们希望将数据从$n$维降至$k$维，我们只需要从$U$中选取前$k$个向量，获得一个$n×k$维度的矩阵，我们用$U_{reduce}$表示，然后通过如下计算获得要求的新特征向量$z^{(i)}$:
> $$z^{(i)}=U^{T}_{reduce}*x^{(i)}$$
>
> 其中$x$是$n×1$维的，因此结果为$k×1$维度。注，我们不对方差特征进行处理。
>
> 而SVD使用奇异值分解来找出空间V，其中Σ也是一个对角矩阵，不过它对角线上的元素是奇异值，这也是SVD中用来衡量特征上的信息量的指标。$U$和$V^{T}$分别是左奇异矩阵和右奇异矩阵，也都是辅助矩阵。我们使用$U$这个矩阵为我们构造降维后的特征。
>
> ##### 选择主成分的数量
>
> 主要成分分析是减少投射的平均均方误差：
>
> 训练集的方差为：$\dfrac {1}{m}\sum^{m}_{i=1}\left\| x^{\left( i\right) }\right\| ^{2}$
>
> 我们希望在平均均方误差与训练集方差的比例尽可能小的情况下选择尽可能小的$k$值。
>
> 如果我们希望这个比例小于1%，就意味着原本数据的偏差有99%都保留下来了，如果我们选择保留95%的偏差，便能非常显著地降低模型中特征的维度了。
>
> 我们可以先令$k=1$，然后进行主要成分分析，获得$U_{reduce}$和$z$，然后计算比例是否小于1%。如果不是的话再令$k=2$，如此类推，直到找到可以使得比例小于1%的最小$k$ 值（原因是各个特征之间通常情况存在某种相关性）。
>
> 还有一些更好的方式来选择$k$，当我们在调用“**svd**”函数的时候，我们获得三个参数：`[U, S, V] = svd(sigma)`。
>
> ![a4477d787f876ae4e72cb416a2cb0b8a](https://tva1.sinaimg.cn/large/008eGmZEgy1gmhfmtw8xwj3065031mx0.jpg)
>
> 其中的$S$是一个$n×n$的矩阵，只有对角线上有值，而其它单元都是0，我们可以使用这个矩阵来计算平均均方误差与训练集方差的比例：
> $$\dfrac {\dfrac {1}{m}\sum^{m}_{i=1}\left\| x^{\left( i\right) }-x^{\left( i\right) }_{approx}\right\| ^{2}}{\dfrac {1}{m}\sum^{m}_{i=1}\left\| x^{(i)}\right\| ^{2}}=1-\dfrac {\Sigma^{m}_{i=1}S_{ii}}{\Sigma^{k}_{i=1}S_{ii}}\leq 1\%$$
>
> 也就是：$$\frac {\Sigma^{k}_{i=1}s_{ii}}{\Sigma^{n}_{i=1}s_{ii}}\geq0.99$$
>
> 在压缩过数据后，我们可以采用如下方法来近似地获得原有的特征：$$x^{\left( i\right) }_{approx}=U_{reduce}z^{(i)}$$



**PCA**作为压缩算法。在那里你可能需要把1000维的数据压缩100维特征，或具有三维数据压缩到一二维表示。所以，如果这是一个压缩算法，应该能回到这个压缩表示，回到你原有的高维数据的一种近似。

所以，给定的$z^{(i)}$，这可能100维，怎么回到你原来的表示$x^{(i)}$，这可能是1000维的数组？

![0a4edcb9c0d0a3812a50b3e95ef3912a](https://tva1.sinaimg.cn/large/008eGmZEgy1gmhfko9gzcj30go09r0v2.jpg)

**PCA**算法，我们可能有一个这样的样本。如图中样本$x^{(1)}$,$x^{(2)}$。我们做的是，我们把这些样本投射到图中这个一维平面。然后现在我们需要只使用一个实数，比如$z^{(1)}$，指定这些点的位置后他们被投射到这一个三维曲面。给定一个点$z^{(1)}$，我们怎么能回去这个原始的二维空间呢？$x$为2维，z为1维，$z=U^{T}_{reduce}x$，相反的方程为：$x_{appox}=U_{reduce}\cdot z$,$x_{appox}\approx x$。如图：

![](https://tva1.sinaimg.cn/large/008eGmZEgy1gmhfl1qo91j30go09n0vi.jpg)

如你所知，这是一个漂亮的与原始数据相当相似。所以，这就是你从低维表示$z$回到未压缩的表示。我们得到的数据的一个之间你的原始数据 $x$，我们也把这个过程称为重建原始数据。

### 3.4 通过规范化变换数据

令A是数值属性，具有n个观测值$v_1,v_2,...,v_n$

##### 最小-最大规范化

把A的值$v_i$映射到**新的区间$[new\_min_{A},new\_max_{A}]$**中的$v_i^{'}$

![](https://tva1.sinaimg.cn/large/008eGmZEgy1gmhgrj9wf7j312g07m0ug.jpg)

##### z分数（z-score）规范化

或零均值规范化，属性$\bar{A}$的值基于$A$的均值和标准差规范化
$$
v^{'} = \frac{v_i - \bar{A}}{\delta_{A}}
$$

##### 小数定标规范化

通过移动属性A的值的小数点位置进行规范化。小数点的移动位数依赖于A的最大绝对值，A的值$v_i$被规范化为$v_i^{'}$,又下式计算
$$
v_i^{'} = \frac{v_i}{10^j}
$$
j是使得$\max(|v_i^{'}|) < 1$的最小整数

## 4 数据仓库

基本概念：

- cell**（单元格）**cuboid**（方体）**cube**（立方体）**

- *数据立方体：维度，度量，格*

- *度量：分布的、代数的、整体的*

基本操作（上卷、下钻、切片、切块）

 数据立方体物化

- *全物化，半物化*
- *聚集路径的选择，优化 （例题）*

### 4.1 数据立方体（data cube ）

允许以多维对数据建模和观察，由维和事实定义。

![](https://tva1.sinaimg.cn/large/008eGmZEgy1gmhi42l1hdj30va0neaek.jpg)

图4.3所示的数据立方体叫做方体(cubiod)，我们对给定的诸维的每个可能的子集产生一个方体，结果形成一个个的格（cell），这个格就是一个个具体的数值的在立方体中的体现。

![](https://tva1.sinaimg.cn/large/008eGmZEgy1gmhi07jhboj315c0nkafn.jpg)

上图中，每个方体代表一个不同程度的汇总，存放最底层汇总的方体叫做**基本方体**，上图是4-D方体，逐渐往上是3-D方体；0-D方体存放最高层的汇总，叫做**顶点方体**，通常用all表示。

### 4.2 多维数据模式

#### 星形模式

最常见的模型范式，其中数据仓库包括

- 一个大的中心表（事实表）,包含大批数据且不含冗余
- 一组小的附属表（维表）每维一个，可以在这里有冗余

![image-20210109161103818](https://tva1.sinaimg.cn/large/008eGmZEgy1gmhiee4zp0j312w0o2q7u.jpg)

#### 雪花模式

星形模式的变种，不同之处在于雪花模式的维表可能是规范化形式，以便减少冗余。这种表易于维护且节省存储空间，但是查询需要更多连接操作，可能降低性能。

![image-20210109161307121](https://tva1.sinaimg.cn/large/008eGmZEgy1gmhighfbv5j31600ok7a1.jpg)

#### 事实星座

复杂的应用，可能需要多个事实表共享维表，这种模式可以看做星形模式的汇集，

![](https://upload-images.jianshu.io/upload_images/14301043-edf222c0132be441.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上图中有两个事实表sales和shipping，共享了维表time,item,location 

### 4.3 概念分层

定义一个映射序列，将低层概念集映射到较高层

![image-20210109162222043](https://tva1.sinaimg.cn/large/008eGmZEgy1gmhiq3yiksj315i0myn0n.jpg)

### 4.4 度量的分类和计算

立方体的度量是一个数值函数，可以对立方体的空间的每个点求值。

根据聚集函数可以分成三类：分布的（distributice）、代数的（algebraic）、整体的（holistic）

- 分布式的

  如果一个聚集函数可以用于分布式的计算，将数据划分成n个集合，将函数用于每个部分，得到n个聚集值，得到的结果和用函数作用于整个数据集的结果一样的话，该函数可以用于分布式计算。如 sum(),count(),min(),max()。**一个度量如果可以用分布聚集函数得到，则它是分布式的**

- 代数的

  一个聚集函数能够用一个具有M个参数的代数函数计算，而每个参数都可以用**一个分布聚集函数**求得，则他是代数的。如avg(),可以用sum()/count()， min_N()、max_N() （找到N个最小最大值）, standard_deviation()。**一个度量如果可以用代数聚集函数得到，则它是代数的。**

- 整体的

  一个聚集函数如果描述它的自己所需的存储没有一个常数界，则它是整体的。也即，不存在一个具有M个参数的代数函数进行这一计算。如 median()、mode()和rank()。**一个度量如果用整体聚集函数得到的，则它是整体的。**

### 4.5 典型的OLAP操作

#### 上卷（roll-up）/ 上钻（drill-up）

通过沿一个维的概念分层向上攀升或者通过维规约在数据立方体上进行聚集

#### 下钻（drill-down）

是上卷的逆操作，它由不太详细的数据得到比较详细的数据。通过沿维的概念分层向下或引入附加的维来实现。

#### 切片（slice）

在给定的立方体的一个维上进行选择，定义子立方体。

#### 切块

通过在两个或多个维上进行选择，定义子立方体。

#### 转轴（pivot）/旋转（rotate）

转动数据的视角，提供数据的替代显示

![image-20210111193307303](https://tva1.sinaimg.cn/large/008eGmZEgy1gmjzh7vnx0j30u00wewps.jpg)

### 4.6 数据立方体的物化

![](https://tva1.sinaimg.cn/large/008eGmZEgy1gmknnwb4isj30jw0k8q50.jpg)

#### compute cube 操作和维灾难

Compute cube操作在指定的维的所有子集上计算聚集。可能需要很大的存储空间。

对于n维立方体的方体总数为$2^n$。

#### 全物化

预先计算所有的方体。通常需要海量的存储。

#### 部分物化

有选择地计算整个可能的方体集中一个适当的子集。

#### 冰山立方体

只存放其聚集值如（count）大于某个最小支持度阈值的立方体单元。这个阈值为最小支持度。

#### 基本单元和聚集单元

基本方体的单元是基本单元，非基本方体的单元是聚集单元。基本方体泛化程度最低，逐层提高，最高的是顶点放图all。

数据立方体其实就是在大量的多维数据中，进行了一个group by的操作，使得数据能够按照一定的规则聚集起来，从而形成一些小的立方体，继而观察各种聚集的值。

聚集单元在一个或多个维度上聚集，其中每个聚集维用单元记号中的 **"\*"** 表示。假设我们有一个n维数据立方体。 令 $a = (a_1,a_2,...,a_n,measures)$是一个单元，取自构成数据立方体的一个方体，如果${a_1,a_2,...,a_n}$恰有$m(m<=n)$个值不是**“\*”**，则我们说$a$是m维单元。m=n是基本单元，m<n是聚集单元。

#### 祖先和后代

单元之间存在祖先后代的关系。在这里我们说一个 $i-D$单元是一个$j-D$单元的祖先，当且仅当

1.  $i<j$且

2. 对于 $1 \geq k \geq n$,只要$a_k \neq *$，就有$a_k = b_k$，特别地，$a$是$b$的父母，且 $b$是 $a$的子女，当且仅当 $j=i+1$。

祖先是后代在某些维上的上卷操作，不考虑某些维或某些维已被上卷；而后代，则是祖先在某些唯上的下钻操作，更细致的去考虑某些维。因此祖先是后代的泛化，后代是祖先的具体。

#### 闭覆盖（closed coverage）

一个单元 c 是闭单元，如果不存在单元d，使得 d 是单元 c 的后代，即d通过将 c 的 * 用非“*”值替换能得到，且d和c有相同的度量。闭立方体是一个仅由闭单元组成的数据立方体。

![](https://tva1.sinaimg.cn/large/008eGmZEgy1gmkkz89inlj30om0gogmz.jpg)



#### 立方体外壳（cube shell）

部分物化的另一个策略是只预计算涉及少数维的方体，这些方体形成对应数据立方体的立方体外壳，在附加的维组合上的查询必须临时计算。



### 4.7 数据立方体的计算

#### 多路数组聚集（MultiWay）

多路数组聚集（MultiWay）方法使用多维数组作为基本的数据结构，计算完全数据立方体。MultiWay是一种使用**数组直接寻址**的典型MOLAP方法，其中维值通过位置或对应数组位置的下标访问，不能使用基于值的重新排序作为优化技术。基于数组的立方体结构构造方法：

a.**把数组分成块**。块是一个子立方体，足够小，可以放入立方体计算时可用的内存。分块是一种把n维数组划分成小的n维块的方法，其中每个块作为一个对象存放在磁盘上。块被压缩，以避免空数组单元所导致的空间浪费。一个单元为空，如果它不含有任何有效数据（其单元计数为零）。如为了压缩稀疏数组结构，在块内搜索单元时可以用chunkID+offset作为单元的寻址机制。

b.**通过访问立方体单元（即访问立方体单元的值）来计算聚集**。可以优化访问单元的次序，使得每个单元必须重复访问的次数最小化，从而减少内存访问开销和存储开销。技巧是使用这样的一种次序，使得多个方体的聚集单元可以同时计算，避免不必要的单元再次访问。

由于分块技术涉及重叠某些聚集计算，因此称该技术未多路数组聚集（multiway array aggregation），执行同时聚集，即同时在多个维组合上计算聚集。

MultiWay使用直接数组寻址，比ROLAP基于关键字的寻址搜索策略快，不过MultiWay计算从基本方体开始，逐步向上到更泛化的祖先方体，因此不能利用先验剪枝。

>计算方法
>
>![](https://tva1.sinaimg.cn/large/008eGmZEgy1gmkoobimjpj319m0lix35.jpg)
>
><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gmkpuse0o3j30rs0vcali.jpg" alt="IMG_EED14BD9C985-1" style="zoom:50%;" />
>
>根据前提条件，如果我们按顺序从1到64依次扫描，BC、AC、AB各有16个块，聚集$b_0c_0$块需要扫描  1-4块，聚集$a_0c_0$则需要扫描1-13块，也就是扫描1、5、7、13块，而对于$a_0,b_0$就需要扫描49块才能聚集了。
>
>我们来计算一下按顺序扫描的话的内存消耗，A、B、C的大小分别为40、400、4000，因此最大的2-D平面是BC（400x4000 = 1600000）,AC(40x400=160000),AB（40x400 = 16000）。按顺序扫描的时候，扫描ABC方体的块1到块4的行，就能聚集一个BC方体的块，相对的，AB需要扫描的块最多。我们的目的是节省内存，则显然我们需要尽可能少的扫描最大的平面。
>
>为了避免把1个3-D块多次调入内存，我们在块内存中维持所有相关2-D平面所需的最小内存单位为：
>
>40 x 400(用于整个AB平面) + 40 x 1000(用于AC平面的一行)+100x1000(用于BC平面的一块) = 156000个内存单位
>
>整个扫描顺序就是先聚集BC，再是 AC，再是AB。
>
>如果我们换一种次序，假设顺序为1、17、33、49、5、21、37、53等，也就是先向AB平面，然后向AC平面，最后向BC平面聚集，则需求量为 400 x 4000(用于整个BC平面) + 10 x 4000(用于AC平面的一行) +10 x 100(用于AB平面的一块)  = 1641000个单位
>
><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gmkpz1njimj30hc0iqdgy.jpg" alt="image-20210112104953271" style="zoom:50%;" />
>
>上图演示了我们如何从ABC聚集到ALL，图**从自底向上**看，每次聚集都由所需内存小的上一方体聚集。一开始AB、AC、BC都由ABC聚集而成，由于AB所需内存最小，则A 和B 由AB聚集而成，C由AC聚集而成，ALL由A聚集而成

#### BUC：从顶点方体向下计算冰山立方体

BUC是一种计算稀疏冰山立方体的算法。和MultiWay不同，BUC从顶点方体向下到基本方体构造冰山立方体，这使得BUC可以分担数据划分开销，这种处理次序也使得BUC在构造立方体时**使用先验性质进行剪枝。**

方体格一般采用顶点方体在顶部基本方体在底部的表示，将下钻（从高聚集单元向较低、更细化的单元移动）和上卷（从细节的、低层单元向较高层、更聚集的单元移动）概念一致起来。BUC是指**自底向上**构造（Bottom-Up Construction），BUC作者采用**顶点方体在底部而基本方体在顶部**的表示，这样看BUC确实是自底向上的。在这里，下钻表示从顶点方体向下到基本方体，所以我们将BUC的探查过程视为自顶向下（这里的下钻方向就反过来了）。<img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gmkqgte5nbj31530rsdpd.jpg" alt="IMG_4C53972C44CE-1" style="zoom:50%;" />

#### Star-Cubing：使用动态星树结构计算冰山立方体

> ​	不用细看

Star-Cubing集成自顶向下和自底向上立方体计算，并利用多维聚集（类似MultiWay）和类Apriori剪枝（类似BUC），在一个称为星树（star-tree）的数据结构上操作，对该数据结构进行无损数据压缩，从而降低计算时间和内存需求量。

Star-Cubing算法利用自底向上和自顶向下的计算模式：在全局计算次序上，使用自底向上模式；同时有一个基于自顶向下模式的子层，利用共享维的概念。如果共享维上的聚集值不满足冰山条件，则沿该共享维向下的所有单元也不可能满足冰山条件。

> ​	共享维：ACD/A意味方体ACD具有共享维A，ABD/AB意味着方体ABD具有共享维AB，ABC/ABC意味方体ABC具有共享维ABC等。
>
> 这源于泛化，在以ACD为根的所有子树中的所有方体都包含维A，在以ABD为根的所有子树中的所有方体都包含维AB，在以ABCX为根的所有子树中的所有方体都包含维ABC（尽管只有一个），我们称这些公共维为特定子树的共享维（shared dimension）
>
> ![](https://tva1.sinaimg.cn/large/008eGmZEgy1gmkqo0087zj30us0j4jtv.jpg)

几个概念：方体数、星节点、星树

**方体树**（cuboid tree），树的每一层代表一个维，每个结点代表一个属性值；每个结点有四个字段：属性值、聚集值、指向第一个子女的指针和指向第一个兄弟的指标；方体中的元组逐个插入组中，一条从根到树叶结点的路径代表一个元组。如果单个维在属性值p上的聚集不满足冰山条件，则在冰山立方体计算中识别这样的节点没有意义。这样的结点p用\*替换，使方体树可以进一步压缩。

> ​	<img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gmkqvey7gpj30uq0f43zy.jpg" style="zoom:50%;" />
>
> 如图所示基本方体ABCD的方体树片段，$c_2$具有聚集（计数）值5，表示$(a_1,b_1,c_2,*)$有5个单元。这种表示合并了公共前缀，节省内存并允许聚集内部结点上的值。利用内部结点上的聚集值，可以进行基于共享维的剪枝，例如AB的方体树可以用来对ABD的可能单元进行剪枝。

**星节点和星树**：如果单个维p上的聚集不满足冰山条件，则称属性A中的结点p是星结点（star node）；否则，称p为非星结点（non-star node）。使用星结点压缩的方体树称为星树（star-tree）。

### 4.8 课后练习

![image-20210112162730137](https://tva1.sinaimg.cn/large/008eGmZEgy1gmkzqiqwrvj31n60u0dxo.jpg)

> ![](https://tva1.sinaimg.cn/large/008eGmZEgy1gmkupdxhy0j31630u04qp.jpg)
>
> 第一问，问立方体有几个非空的方体，就是最底层是10维的基本方体，一层一层聚集上去之后，一共有几个，是$2^{10}$个。我们题目给的数据是基本方体的具体的格，两者是一种概念和实体的关系。
>
> 第二问，求非空的经过3个基本格聚集的格有多少个。题目给的格在数据立方体中是基本方体，每个格可以向上聚集$2^{10}-1$个格，即将其中的数据p一个个的变成\*。总的格数为$3\times2^{10}-3$。
>
> 往上聚集时候必会有重叠的部分，相异的部分一定在前三个确定的情况下，其中前三个为\*的可以分别由三个基本单元格聚集而成，它们的度量值为3，每个出现了$2^7$次，因此需要减掉$2 \times 2^7$个单元格即$(*,*,*,d_4,...,d_{10})$，；另外$(*,d_2,*,...,*)、(d_1,*,...,*)、(*,*,d_3,*,...,*)$各出现了两次合计度量值为6，则只取出现的1次，所以还要减掉$3\times2^7$个单元格。
>
> 所以总的单元格为$3\times2^{10}-3 - 2 \times 2^7 - 3\times2^{7}$ 
>
> 第三问，求聚集单元格大于等于2的数量，由第二问知，$(*,d_2,*,...,*)、(d_1,*,...,*)、(*,*,d_3,*,...,*)$符合，它们各出现两次，$(*,*,*,d_4,...,d_{10})$出现了三次，也符合，所以一共有$4\times2^7$。
>
> 第四问，一个单元 c 是闭单元，如果不存在单元d，使得 d 是单元 c 的后代，即d通过将 c 的 * 用非“*”值替换能得到，且**d和c有相同的度量**。闭立方体是一个仅由闭单元组成的数据立方体。闭单元要和祖先和后代单元的概念结合看。
>
> 本题问在全物化的数据立方体中闭单元的个数，则是答案中的7个。首先三个基本单元是闭单元，对于$(*,*,d_3,d_4,...,d_{10})$$(*,d_2,*,d_4,...,d_{10})$$(d_1,*,*,d_4,...,d_{10})$而言，度量值为2，当用 a或b或c去替换对应的\*,度量值都变了，变成了1，最后一个单元格$(*,*,*,d_4,...,d_{10})$度量值为3，用a或b或c去替换\*，度量值也变了，变成了1，因此也是闭单元格。

![](https://tva1.sinaimg.cn/large/008eGmZEgy1gml32dbq8lj31af0u0th7.jpg)

> ​	![](https://tva1.sinaimg.cn/large/008eGmZEgy1gmkzr3uxwtj315q0u044c.jpg)
>
> 对于(C)的多路计算，自底向上计算，AC数量级最大，AB次之，BC最小，因此B和C由BC聚集，A由AB聚集

## 5. 关联规则

### 5.1 关联规则

– *支持度和置信度的定义和计算*

 ### 5.2 Apriori算法的原理及实现

– *K-项集*

– *极大频繁项集、闭频繁项集的概念*

### 5.3 FP-growth算法的原理及实现

 量化关联规则（对Apriori算法的简单扩展，了解原理）

## 6. 分类：概念、决策树、最近邻、贝叶斯、集成学习

### 6.1 分类的概念

### 6.2 决策树(原理)

- *什么优点 有什么缺点*

- *原理及实现*

### 6.3 混淆矩阵（精度、召回率计算）

### 6.4 最近邻（原理） 、贝叶斯（原理） 、支持向量机（原理）

### 6.5 集成学习（原理，为什么好于单分类器）

## 7. 聚类：概念、划分聚类、层次聚类

### 7.1 聚类概念

- *几种聚类间距离计算(**平均值，最大最小距离，期望值等**)* 

- *聚类质量评价方法*

### 7.2 K-means、K-Medoids 聚类方法（原理，算法，优化，优缺点）

- *贪心策略与全局最优*

- *参数选择*

### 7.3 CF-Tree，BIRCH算法（原理，优缺点）

### 7.4 DBSCAN算法（原理）

